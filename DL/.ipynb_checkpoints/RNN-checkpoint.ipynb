{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fname = './Onegin/onegin.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_CHAR  = '\\b'\n",
    "END_CHAR = '\\t'\n",
    "PADDING_CHAR = '\\a'\n",
    "chars = set([START_CHAR,'\\n',END_CHAR])\n",
    "with open(input_fname,'r') as f:\n",
    "    for line in f:\n",
    "        chars.update(list(line.strip().lower()))\n",
    "char_indices = {c: i for i,c in enumerate(sorted(list(chars)))}\n",
    "char_indices[PADDING_CHAR] = 0\n",
    "indices_to_chars = {i:c for c,i  in char_indices.items()}\n",
    "num_chars = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one(i, sz):\n",
    "    res = np.zeros(sz)\n",
    "    res[i] = 1\n",
    "    return res\n",
    "\n",
    "char_vectors = {\n",
    "    c:(np.zeros(num_chars) if c==PADDING_CAHR else get_one(v,num_chars))\n",
    "    for c,v in char_indices.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_end_markers = set('.?!')\n",
    "sentences = []\n",
    "current_sentence = ''\n",
    "with open(input_fname, 'r') as f:\n",
    "    for line in f:\n",
    "        s = line.strip().lower()\n",
    "        if len(s) > 0:\n",
    "            current_sentence+=s +'\\n'\n",
    "        if len(s) == 0 or s[-1] in sentence_end_markers:\n",
    "            current_sentence = current_sentence.strip()\n",
    "            if len(current_sentence) > 10:\n",
    "                sentences.append(current_sentence)\n",
    "            current_sentence = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(sentences):\n",
    "    max_sentence_len = np.max([len(x) for x in sentences])\n",
    "    X = np.zeros((len(sentences), max_sentence_len, len(chars)),dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), max_sentence_len, len(chars)),dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        char_seq = (START_CHAR + sentence + END_CHAR).ljust( max_sentence_len+1,\n",
    "                                                           PADDING_CHAR\n",
    "                                                           )\n",
    "        for t in range(max_sentence_len):\n",
    "            X[i,t,:] = char_vectors[char_seq[t]]\n",
    "            y[i,t,:] = char_vectors[char_seq[t+1]]\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, TimeDistributed, Activation\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(output_dim=128, \n",
    "               activation = 'tanh',\n",
    "               return_sequences=True,input_dim = num_chars))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(output_dim=num_chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer = Adam(clipnorm=1.),\n",
    "             metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = np.random.choice(range(len(sentences)), int(len(sentences)*0.05))\n",
    "sentences_train = [\n",
    "    sentences[x] for x in set(range(len(sentences))) - set(test_indices)\n",
    "]\n",
    "sentences_test = [sentences[x] for x in test_indices]\n",
    "X_test, y_test = get_matrix(sentences_test)\n",
    "batch_size  = 16\n",
    "def generate_batch():\n",
    "    while True:\n",
    "        for i in range( int(len(sentences_train)/batch_size) ):\n",
    "            sentences_batch = sentences_train[i*batch_size:(i+1)*batch_size]\n",
    "            yield get_matrix(sentences_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "import os \n",
    "output_file = 'out'\n",
    "class CharSampler(Callback):\n",
    "    \n",
    "    def __init__(self,char_vectors, model):\n",
    "        self.char_vectors = char_vectors\n",
    "        self.model = model\n",
    "        \n",
    "    def on_train_begin(self,logs={}):\n",
    "        self.epoch = 0\n",
    "        if os.path.isfile(output_file):\n",
    "            os.remove(output_file)\n",
    "    \n",
    "    def sample(self,preds,temperatue=1.0):\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds)/temperatue\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds/np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1,preds,1)\n",
    "        return np.argmax(probas)\n",
    "    \n",
    "    def sample_one(self, T):\n",
    "        result = START_CHAR\n",
    "        while len(result) < 500:\n",
    "            Xsampled = np.zeros((1,len(result), num_chars))\n",
    "            for t,c in enumerate(result):\n",
    "                Xsampled[0,t,:] = self.char_vectors[c]\n",
    "            ysampled= self.model.predict(Xsampled, batch_size=1)[0,:]\n",
    "            yv = ysampled[len(result)-1,:]\n",
    "            selected_char = indices_to_chars[self.sample(yv,T)]\n",
    "            if selected_char == END_CHAR:\n",
    "                break\n",
    "            result  = result + selected_char\n",
    "        return result\n",
    "    \n",
    "    def one_epoch_end(self,batch,logs={}):\n",
    "        self.epoch = self.epoch+1\n",
    "        if self.epoch %50==0:\n",
    "            print(f'\\n Epoch {self.epoch}  text sampling:')\n",
    "            with open(output_file, 'a') as outf:\n",
    "                outf.write(f'\\n====Epoch {self.epoch} ====')\n",
    "                for T in [0.3,0.5,0.7,0.9,1.1]:\n",
    "                    print(f'\\tsampling, T = {T}')\n",
    "                    for _ in range(5):\n",
    "                        self.model.reset_states()\n",
    "                        res = self.sample_one(T)\n",
    "                        outf.write(f'\\nT = {T} = {res}')\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "model_fname = 'LSTM'\n",
    "\n",
    "cb_sampler = CharSampler(char_vectors, model)\n",
    "cb_logger = CSVLogger('./' + model_fname + '.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generate_batch(),\n",
    "                   int(len(sentences)/batch_size)*batch_size,\n",
    "                   nb_epoch=1,verbose=True,validation_data=(X_test,y_test),\n",
    "                   callbacks=[cb_logger,cb_sampler]\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import merge, Input\n",
    "\n",
    "vec = Input(shape=[None, num_chars])\n",
    "l1 = LSTM(output_dim=128, activation='tanh', return_sequences=True)(vec)\n",
    "l1_d = Dropout(0.2)(l1)\n",
    "\n",
    "input2 = merge([vec,l1_d],mode='concat')\n",
    "l2 = LSTM(output_dim=128, activation='tanh', return_sequences=True)(input2)\n",
    "l2_d = Dropout(0.2)(l2)\n",
    "\n",
    "input3 = merge([vec,l2_d],mode='concat')\n",
    "l3 = LSTM(output_dim=128, activation='tanh', return_sequences=True)(input3)\n",
    "l3_d = Dropout(0.2)(l2)\n",
    "\n",
    "input_d = merge([l1_d,l2_d,l3_d], mode='concat')\n",
    "\n",
    "dense3 = TimeDistributed(\n",
    "    Dense(output_dim=num_chars)\n",
    ")(input_d)\n",
    "output_res = Activation('softmax')(dense3)\n",
    "\n",
    "model = Model(input=vec, output= output_res)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer = Adam(clipnorm=1.),\n",
    "             metrics = ['accuracy'])\n",
    "model.fit_generator(generate_batch(),\n",
    "                   int(len(sentences)/batch_size)*batch_size,\n",
    "                   nb_epoch=1,verbose=True,validation_data=(X_test,y_test),\n",
    "                   callbacks=[cb_logger,cb_sampler]\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
