{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy as sp\n",
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "import stanza\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./datasets/train_sessions.csv')\n",
    "df_test = pd.read_csv('./datasets/test_sessions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = [f'time{i}' for i in range(1,11)]\n",
    "df_train[time] = df_train[time].apply(pd.to_datetime)\n",
    "df_test[time] = df_test[time].apply(pd.to_datetime)\n",
    "df_train_ = df_train.sort_values(by='time1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [f'site{i}' for i in range(1,11)]\n",
    "df_train[sites] = df_train[sites].fillna(0).astype(int).astype(str)\n",
    "df_test[sites] = df_test[sites].fillna(0).astype(int).astype(str)\n",
    "df_train['list'] = df_train['site1']\n",
    "df_test['list'] = df_test['site1']\n",
    "for s in sites[1:]:\n",
    "    df_train['list'] = df_train['list'] + ',' + df_train[s]\n",
    "    df_test['list'] = df_test['list'] + ',' + df_test[s]\n",
    "df_train['w_list'] = df_train['list'].apply(lambda x: x.split(','))\n",
    "df_test['w_list'] = df_test['list'].apply(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['targen'] = -1\n",
    "data = pd.concat([df_train,df_test], axis = 0)\n",
    "\n",
    "model = word2vec.Word2Vec(data['w_list'], size = 300, window=3,workers = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanVectorize:\n",
    "    def __init__(self,w2v):\n",
    "        self.word2vec = w2v\n",
    "        self.dim = len(next(iter(w2v.values())))\n",
    "    \n",
    "    def fit(self,X):\n",
    "        return self\n",
    "    \n",
    "    def tranform(self,X):\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = MeanVectorize(w2v).tranform(data['w_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp =data['target'].fillna(-1)\n",
    "train_idx = (temp == 0) | (temp == 1)\n",
    "test_idx = (temp == -1)\n",
    "X = data_mean[train_idx]\n",
    "Y = temp[train_idx]\n",
    "test = data_mean[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, Y_train, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-13 07:32:58 WARNING: From /home/hvidsmen/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=(Xtr.shape[1])))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-13 07:33:26 WARNING: From /home/hvidsmen/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    class_weight='auto',\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9289532346332854"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test, batch_size=128)\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(X_train, label= y_train,missing = np.nan)\n",
    "d_test = xgb.DMatrix(X_test, label= y_test,missing = np.nan)\n",
    "watchlist = [(d_train, 'train'), (d_test, 'eval')]\n",
    "history = dict(watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.95688\teval-auc:0.87422\n",
      "[20]\ttrain-auc:0.99017\teval-auc:0.92753\n",
      "[40]\ttrain-auc:0.99281\teval-auc:0.93328\n",
      "[60]\ttrain-auc:0.99444\teval-auc:0.93554\n",
      "[80]\ttrain-auc:0.99562\teval-auc:0.93765\n",
      "[100]\ttrain-auc:0.99652\teval-auc:0.93904\n",
      "[120]\ttrain-auc:0.99718\teval-auc:0.94035\n",
      "[140]\ttrain-auc:0.99765\teval-auc:0.94074\n",
      "[160]\ttrain-auc:0.99801\teval-auc:0.94183\n",
      "[180]\ttrain-auc:0.99830\teval-auc:0.94276\n",
      "[199]\ttrain-auc:0.99853\teval-auc:0.94345\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_depth': 26,\n",
    "    'eta': 0.025,\n",
    "    'nthread': 4,\n",
    "    'gamma' : 1,\n",
    "    'alpha' : 1,\n",
    "    'subsample': 0.85,\n",
    "    'eval_metric': ['auc'],\n",
    "    'objective': 'binary:logistic',\n",
    "    'colsample_bytree': 0.9,\n",
    "    'min_child_weight': 100,\n",
    "    'scale_pos_weight':(1)/Y.mean(),\n",
    "    'seed':7\n",
    "}\n",
    "\n",
    "model = xgb.train(params, d_train, num_boost_round=200, evals=watchlist, evals_result=history, verbose_eval=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5077658486187722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1, random_state=7, n_jobs=-1)\n",
    "lr_fit = lr.fit(X_train,y_train)\n",
    "y_pred = lr_fit.predict(X_test)\n",
    "score = roc_auc_score(y_test, y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "class tfidf_vectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(next(iter(w2v.values())))\n",
    "\n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5077658486187722\n"
     ]
    }
   ],
   "source": [
    "data_mean = tfidf_vectorizer(w2v).fit(data['w_list']).transform(data['w_list'])\n",
    "\n",
    "temp =data['target'].fillna(-1)\n",
    "train_idx = (temp == 0) | (temp == 1)\n",
    "test_idx = (temp == -1)\n",
    "X = data_mean[train_idx]\n",
    "Y = temp[train_idx]\n",
    "test = data_mean[test_idx]\n",
    "\n",
    "lf_fit = lr.fit(X_train,y_train)\n",
    "y_pred = lr_fit.predict(X_test)\n",
    "score = roc_auc_score(y_test, y_pred)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
